---
title: "L1 and L2 reguralization"
author: "LK"
date: "2022-10-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Getting the data

```{r}
library(tidymodels)
library(ISLR)

Hitters <- as_tibble(Hitters) %>%
  filter(!is.na(Salary))

Hitters
```

# Creating a specification

```{r}
ridge_spec <- linear_reg(mixture = 0, penalty = 0) %>% #set mixture = 0 for ridge regularization and mixture = 1 for lasso regularization
  set_mode("regression") %>%
  set_engine("glmnet") #performs the ridge regularization

ridge_spec
```
# Fitting to our data

```{r}
options(scipen=999, digits = 1)

ridge_fit <- fit(ridge_spec, Salary ~ ., data = Hitters)

tidy(ridge_fit)
```
# Increasing the L2 penalty tterm to see the effect on the estimates

```{r}
tidy(ridge_fit, penalty = 705)
tidy(ridge_fit, penalty = 50)
```
#Visualize magnitude on coefficients as PENALTY GOES UP
```{r}
ridge_fit %>%
  autoplot()
```

# Prediction when the penalty term is 0
```{r}
predict(ridge_fit, new_data = Hitters)
```

# Predictions when we increase the penalty term to 500

```{r}
predict(ridge_fit, new_data = Hitters, penalty = 500)
```

#Finding the best value of Lambda using hyperparameter tuning

```{r}
Hitters_split <- initial_split(Hitters, strata = "Salary")

Hitters_train <- training(Hitters_split)
Hitters_test <- testing(Hitters_split)

Hitters_fold <- vfold_cv(Hitters_train, v = 10)#Kfold cross validation
```


Before hyperparameter tuning, we need to normalize the data as ridge regression is scale sensitive

# Specifying the recipe

```{r}
ridge_recipe <- 
  recipe(formula = Salary ~ ., data = Hitters_train) %>% 
  step_novel(all_nominal_predictors()) %>% #assign a previously unseen factor level to a new value.
  step_dummy(all_nominal_predictors()) %>% #Encode factor variables
  step_zv(all_predictors()) %>% #creates a specification of a recipe step that will remove variables that contain only a single value/ potentially remove columns from the data set.
  step_normalize(all_predictors())
```

# Model specification with a tune prameter on the penalty/lamda

```{r}
ridge_spec <- 
  linear_reg(penalty = tune(), mixture = 0) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")
```


# Create a workflow

```{r}
ridge_workflow <- workflow() %>% 
  add_recipe(ridge_recipe) %>% 
  add_model(ridge_spec)
```

# Getting the value of the pena√∂ty term/Lambda

```{r}
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)
penalty_grid
```
```{r}
tune_res <- tune_grid( #computes a set of performance metrics (e.g. accuracy or RMSE)
  ridge_workflow,
  resamples = Hitters_fold, 
  grid = penalty_grid
)

tune_res
```
# Plot

```{r}
autoplot(tune_res)
```
# Raw metrics
```{r}
collect_metrics(tune_res)
```
# Select the best penalty term
```{r}
best_penalty <- select_best(tune_res, metric = "rsq")
best_penalty
```

# update the recipe with the best penalty term

```{r}
ridge_final <- finalize_workflow(ridge_workflow, best_penalty)

ridge_final_fit <- fit(ridge_final, data = Hitters_train)
```

# Apply model on testing data set to assess performance

```{r}
fit <- augment(ridge_final_fit, new_data = Hitters_test) %>% 
    rsq(truth = Salary, estimate = .pred) 
fit
```

